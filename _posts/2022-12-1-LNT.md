# 大数定律和中心极限定律

## 1. 何谓依概率收敛

### 1. 定义

$$P(|X-C|<\varepsilon )\to 1$$收敛速度与 $\varepsilon$ 有关

### 2. 性质

1. 区别于 almost surely 收敛(几乎处处收敛)，
   $$P(X_n \to X)\to 1$$
此处所说的$a.s.$收敛需要$|X_n-X|$的取值均在$\varepsilon$内，除开一个零测度集，而依概率收敛则可以满足上述随机变量的一些取值(取值构成一个任意小的测度集)不在$\varepsilon$内，这也就说明了为什么不直接将频率定义为概率。

2. 此外还有依分布收敛，和上面所说的两个收敛的关系(由强到弱)  
   $$a.s.\Longrightarrow P\Longrightarrow  L$$
依分布收敛指的是随机变量$X_n$的分布同$X$，若$n\to \infty$.这等价于特征函数
$$\varphi_{X_n}(\theta)=\varphi_{X}(\theta),\quad n\to \infty$$
当收敛到常数的时候，依概率收敛和依分布收敛等价，否则只能前者推后者。

3. __(Slusky's Theorem)__ 若 $X_{n} \stackrel{D}{\longrightarrow} X, Y_{n} \stackrel{P}{\longrightarrow} a$ ,则 $X_{n}+Y_{n} \stackrel{D}{\longrightarrow} X+a$

## 大数定律

### 结论
   $$\lim_{n\to \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}\xi_k-\frac{1}{n}\sum_{k=1}^{n}E\xi_k|<\varepsilon \}=1$$ 

### 各种条件

#### 切比雪夫大数定律

   1. 条件：$\xi_1\cdots \xi_n$ 两两不相关，且方差一致有界。


   2. 引理：切比雪夫不等式：
         1. 若随机变量$X$有二阶矩，那么如下关系成立
$$P\{|X-c|>\varepsilon\}\le \frac{DX}{\varepsilon^2}$$

#### 辛钦大数定律

   1. 条件：要求 $\xi_1\cdots \xi_n$ 独立同分布且 $E\xi_i$ 有限，则 $$\lim_{n\to \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}\xi_k-E\xi|<\varepsilon \}=1$$

   2. 此结论要比切比雪夫定律来得稍微好一点，容易使用...

   3. __推论: (Bernoulli 大数定律)__  在事件 A 发生的概率为 p 的 n 次重复独立试验的 Bernoulli 概型中，令 $\mu_n$ 为 n 次重复中试验 A 发生的次数, 有 $\frac{\mu_n}{n}\stackrel{P}{\longrightarrow} \mu$

### 中心极限定理

#### 定义

 对于任意 $X_n$, 我们有 $$\lim _{n \rightarrow \infty} P\left(\frac{\sum_{i=1}^{n} X_{i}-\sum_{i=1}^{n} E X_{i}}{\sqrt{D\left(\sum_{i=1}^{n} X_{i}\right)}}<x\right)=\Phi(x) \quad \forall x \in R$$

 换句话说，就是
 $$\frac{\sum_{i=1}^{n} X_{i}-\sum_{i=1}^{n} E X_{i}}{\sqrt{D\left(\sum_{i=1}^{n} X_{i}\right)}}\stackrel{D}{\longrightarrow} N(0,1) $$
 即当 $n\to \infty$ 时，$\sum_{n=1}^{\infty}X_n\to N(0,1) $

#### 各类定理

1. __(Levy-Lindeberg)__  $X_i$ 独立同分布，且有有限的期望 $EX_i=\mu$ 和方差 $DX_i=\sigma^2$. 那么 $$\frac{\sum_{i=1}^{n} X_{i}-n\mu}{\sqrt{n}\sigma}\stackrel{D}{\longrightarrow} N(0,1)$$

2. 用于独立和的近似计算，比如二项分布的和等等.