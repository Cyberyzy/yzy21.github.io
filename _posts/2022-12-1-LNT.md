<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
## 大数定律和中心极限定律

### 1. 何谓依概率收敛

   定义是这样的：$$P(|X-C|<\varepsilon )\to 1$$收敛速度与$\varepsilon$有关
   区别于almost surely 收敛(几乎处处收敛)，
   $$P(X_n \to X)\to 1$$
此处所说的$a.s.$收敛需要$|X_n-X|$的取值均在$\varepsilon$内，除开一个零测度集，而依概率收敛则可以满足上述随机变量的一些取值(取值构成一个任意小的测度集)不在$\varepsilon$内，这也就说明了为什么不直接将频率定义为概率。

此外还有依分布收敛，和上面所说的两个收敛的关系(由强到弱)$$a.s.\Longrightarrow P\Longrightarrow  L$$
依分布收敛指的是随机变量$X_n$的分布同$X$， 若$n\to \infty$.这等价于特征函数
$$\varphi_{X_n}(\theta)=\varphi_{X}(\theta),\quad n\to \infty$$
当收敛到常数的时候，依概率收敛和依分布收敛等价，否则只能前者推后者。
1. Bernoulli 大数定律
   1. 切比雪夫大数定律
      1. 条件：$\xi_1\cdots \xi_n$两两不相关，且方差一致有界。
      2. 结论$$\lim_{n\to \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}\xi_k-\frac{1}{n}\sum_{k=1}^{n}E\xi_k|<\epsilon \}=1$$
      3. 引理：切比雪夫不等式：
         1. 若随机变量$X$有二阶矩，那么如下关系成立
$$P\{|X-c|>\varepsilon\}\le \frac{DX}{\varepsilon^2}$$

1. 辛钦大数定律
   1. 条件：要求$\xi_1\cdots \xi_n$独立同分布且$E\xi_i$有限，则$$\lim_{n\to \infty}P\{|\frac{1}{n}\sum_{k=1}^{n}\xi_k-E\xi|<\epsilon \}=1$$
   2. 此结论要比切比雪夫定律来得稍微好一点，容易使用...
   
2. 
